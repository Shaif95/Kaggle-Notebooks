{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\n# dicom\nimport pydicom as dicom\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:32:18.145470Z","iopub.execute_input":"2024-05-27T19:32:18.145871Z","iopub.status.idle":"2024-05-27T19:32:18.151833Z","shell.execute_reply.started":"2024-05-27T19:32:18.145840Z","shell.execute_reply":"2024-05-27T19:32:18.150607Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# read data\ndf_train_main = pd.read_csv('../input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\ndf_train_label = pd.read_csv('../input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv')\ndf_train_desc = pd.read_csv('../input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\ndf_test_desc = pd.read_csv('../input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv')\ndf_sub = pd.read_csv('../input/rsna-2024-lumbar-spine-degenerative-classification/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:30:06.816237Z","iopub.execute_input":"2024-05-27T19:30:06.817382Z","iopub.status.idle":"2024-05-27T19:30:06.915262Z","shell.execute_reply.started":"2024-05-27T19:30:06.817339Z","shell.execute_reply":"2024-05-27T19:30:06.914272Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Usando melt para transformar colunas em linhas\ndf_unpivoted = df_train_main.melt(id_vars='study_id', var_name='condition', value_name='status')\nfrequency_table = df_unpivoted.groupby('condition')['status'].value_counts(normalize=True).unstack(fill_value=0)\n\n# Resetando o índice para que 'condition' seja uma coluna novamente\nfrequency_table = frequency_table.reset_index()\n\nfrequency_table.rename(columns={'Moderate': 'moderate', 'Normal/Mild': 'normal_mild', 'Severe': 'severe'}, inplace=True)\ndf_sub['condition'] = df_sub['row_id'].str.extract(r'_(.*)')\ndf_sub = pd.merge(df_sub[['row_id', 'condition']],frequency_table, on='condition', how='inner')[['row_id', 'normal_mild', 'moderate', 'severe']]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T23:35:30.719135Z","iopub.execute_input":"2024-05-22T23:35:30.720213Z","iopub.status.idle":"2024-05-22T23:35:30.794270Z","shell.execute_reply.started":"2024-05-22T23:35:30.720164Z","shell.execute_reply":"2024-05-22T23:35:30.793224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save submission file\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T23:35:46.774034Z","iopub.execute_input":"2024-05-22T23:35:46.775108Z","iopub.status.idle":"2024-05-22T23:35:46.783965Z","shell.execute_reply.started":"2024-05-22T23:35:46.775070Z","shell.execute_reply":"2024-05-22T23:35:46.782605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport pydicom\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Input\nfrom tensorflow.keras.optimizers import Adam\n\n# Load the DataFrame\ndf_train_main = pd.read_csv('../input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\n\n# Define the data generator function\ndef data_generator(df, image_dir, batch_size=10, target_size=(32, 32), images_per_batch=12):\n    # Shuffle the data at the beginning of each epoch\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # Define the label mapping\n    label_mapping = {\n        \"Normal/Mild\": [1, 0, 0],\n        \"Moderate\": [0, 1, 0],\n        \"Severe\": [0, 0, 1]\n    }\n\n    def load_images_from_subfolders(study_path, num_images):\n        images = []\n        subfolders = [f.path for f in os.scandir(study_path) if f.is_dir()]\n        subfolders = sorted(subfolders)[:min(len(subfolders), 4)]  # Consider at most 4 subfolders\n        images_per_folder = max(1, num_images // len(subfolders))\n        \n        for folder in subfolders:\n            files = [f for f in os.listdir(folder) if f.endswith('.dcm')]\n            files = sorted(files)[:images_per_folder]\n            \n            for file in files:\n                image_path = os.path.join(folder, file)\n                # Load DICOM image\n                dicom = pydicom.dcmread(image_path)\n                image = dicom.pixel_array\n                \n                # Convert image to uint8 if necessary\n                if image.dtype != np.uint8:\n                    image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n                image = cv2.resize(image, target_size)\n                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # Convert to RGB\n                \n                images.append(image)\n                \n                if len(images) >= num_images:\n                    break\n            if len(images) >= num_images:\n                break\n                \n        return np.array(images)\n\n    while True:\n        for start in range(0, len(df), batch_size):\n            end = min(start + batch_size, len(df))\n            batch_df = df[start:end]\n            \n            images_batch = []\n            labels_batch = []\n            \n            for _, row in batch_df.iterrows():\n                study_id = row['study_id']\n                original_label = row['spinal_canal_stenosis_l1_l2']\n                one_hot_label = label_mapping[original_label]\n                \n                # Build the path to the study folder and get images\n                study_path = os.path.join(image_dir, str(study_id))\n                images = load_images_from_subfolders(study_path, images_per_batch)\n                \n                # Normalize images\n                images = images.astype('float32') / 255.0\n                \n                images_batch.append(images)\n                labels_batch.append(one_hot_label)\n                \n            images_batch = np.array(images_batch)\n            labels_batch = np.array(labels_batch)\n            \n            yield images_batch, labels_batch\n\n# Define the CNN-LSTM model\ndef create_model(input_shape):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(64))\n    model.add(Dense(3, activation='softmax'))\n    \n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Usage example\nimage_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images'\nbatch_size = 10\ninput_shape = (12, 32, 32, 3)\n\n# Create data generators\ntrain_gen = data_generator(df_train_main[:int(0.9*len(df_train_main))], image_dir, batch_size=batch_size)\nval_gen = data_generator(df_train_main[int(0.9*len(df_train_main)):], image_dir, batch_size=batch_size)\n\n# Create the model\nmodel = create_model(input_shape)\n\n# Train the model\nmodel.fit(train_gen, steps_per_epoch=180, validation_data=val_gen, validation_steps=20, epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:58:19.933794Z","iopub.execute_input":"2024-05-27T19:58:19.934151Z","iopub.status.idle":"2024-05-27T20:04:40.461997Z","shell.execute_reply.started":"2024-05-27T19:58:19.934124Z","shell.execute_reply":"2024-05-27T20:04:40.460713Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m162/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9191 - loss: 0.2758","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(input_shape)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nKeyError: nan\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/generator_data_adapter.py\", line 52, in get_tf_iterator\n    for batch in self.generator:\n\n  File \"/tmp/ipykernel_33/2666337837.py\", line 68, in data_generator\n    one_hot_label = label_mapping[original_label]\n\nKeyError: nan\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_5379]"],"ename":"UnknownError","evalue":"Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nKeyError: nan\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/generator_data_adapter.py\", line 52, in get_tf_iterator\n    for batch in self.generator:\n\n  File \"/tmp/ipykernel_33/2666337837.py\", line 68, in data_generator\n    one_hot_label = label_mapping[original_label]\n\nKeyError: nan\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_5379]","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:52:56.292352Z","iopub.execute_input":"2024-05-27T19:52:56.292761Z","iopub.status.idle":"2024-05-27T19:52:56.477653Z","shell.execute_reply.started":"2024-05-27T19:52:56.292730Z","shell.execute_reply":"2024-05-27T19:52:56.476278Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}